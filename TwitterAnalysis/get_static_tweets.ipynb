{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: Deanna Schneider\n",
    "The following three cells do set up for the work of collecting tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tweepy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving access tokens in a separate csv file that will not be included with the git push so that my credentials stay local.\n",
    "keys = pd.read_csv('keys.csv')\n",
    "\n",
    "con_key = keys.con_key[0]\n",
    "con_secret = keys.con_secret[0]\n",
    "acc_token = keys.acc_token[0]\n",
    "acc_secret = keys.acc_secret[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Use tweepy.OAuthHandler to create an authentication using the given key and secret\n",
    "auth = tweepy.OAuthHandler(consumer_key=con_key, consumer_secret=con_secret)\n",
    "auth.set_access_token(acc_token, acc_secret)\n",
    "\n",
    "#Connect to the Twitter API using the authentication\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Rationale\n",
    "\n",
    "The majority of the work of collecting and storing tweets is done by the following two functions. While I could have done a wrapper class for all of this, it didn't seem necessary, given that there were only two functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tweets(number, hashtag):\n",
    "    \"\"\"\n",
    "    Takes in the number of tweets to return and a single hashtag (without the hash)\n",
    "    Returns a list of tweets, filtering out retweets and replies\n",
    "    \"\"\"\n",
    "\n",
    "    #set the number needed\n",
    "    num_needed = number\n",
    "    tweet_list = []\n",
    "    last_id = -1 # id of last tweet seen\n",
    "    while len(tweet_list) < num_needed:\n",
    "        try:\n",
    "            #https://stackoverflow.com/questions/27941940/how-to-exclude-retweets-and-replies-in-a-search-api\n",
    "            #we are only looking for text-based original tweets, so filtering out links, images and videos\n",
    "            new_tweets = api.search(q = '%23' + hashtag + ' AND lang:en AND -filter:retweets AND -filter:replies AND  -Filter:Links AND -Filter:Media', count = 100,  tweet_mode='extended', max_id = str(last_id - 1))\n",
    "        except tweepy.TweepError as e:\n",
    "            print(\"Error\", e)\n",
    "            break\n",
    "        else:\n",
    "            if not new_tweets:\n",
    "                print(\"Could not find any more tweets!\")\n",
    "                return tweet_list\n",
    "                break\n",
    "            tweet_list.extend(new_tweets)\n",
    "            last_id = new_tweets[-1].id\n",
    "    \n",
    "    return tweet_list       \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_Tweets(filename, tweet_list):\n",
    "    \"\"\"\n",
    "    Takes in a filename and a tweet_list.\n",
    "    Writes the tweets to a file\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename,\"w\", newline=\"\", encoding='utf-8') as tweets:\n",
    "        tweets.write(\"AuthorID|Author.Screen_Name|Followers.Count|Friends.Count|Statuses.Count|Language|Created_At|Favorite_count|Text|Source|retweet_count|Hashtags\\n\")\n",
    "        for t in tweet_list:\n",
    "            hashtags = [h[\"text\"] for h in t.entities[\"hashtags\"]]\n",
    "\n",
    "            tweets.write(\"%(author_id)s|%(authorscreename)s|%(followers_count)s|%(friends_count)s|%(statuses_count)s|%(language)s|%(created_at)s|%(favorite_count)s|%(text)s|%(source)s|%(retweetcount)s|%(hashtags)s\\n\" %\n",
    "                                {'author_id': t.author.id_str,\n",
    "                                     'authorscreename': t.author.screen_name.replace('|','-').replace('\\n', ' ').replace('\\r', ' '), \n",
    "                                     'followers_count': t.author.followers_count,\n",
    "                                     'friends_count': t.author.friends_count,\n",
    "                                     'statuses_count': t.author.statuses_count,\n",
    "                                     'language': t.author.lang, \n",
    "                                     'created_at': t.created_at, \n",
    "                                     'favorite_count': t.favorite_count, \n",
    "                                     'text': t.full_text.replace('|','-').replace('\\n', ' ').replace('\\r', ' ').replace('\"', '').replace(\"'\", \"\"), \n",
    "                                     'source':t.source.replace('|','-').replace('\\n', ' ').replace('\\r', ' ').replace('\"', '').replace(\"'\", \"\"), \n",
    "                                     'retweetcount': t.retweet_count, \n",
    "                                     'hashtags': hashtags })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Tweets\n",
    "Tweets were collected at two points in time - October and December 2017. Initially, I was hard-coding the file name to just hashtag_static.txt. When I realized I needed to collect more tweets, I thought it wise to add a dynamic date component, so that I wouldn't accidentally overwrite previous files and so that I could track when the collection took place. The initial tweets are still stored in the repository with the static file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MeToo - a passive voice tweet\n",
    "\n",
    "#get the date\n",
    "now = datetime.datetime.now()\n",
    "#set the filename\n",
    "filename = \"metoo_static_%d_%d_%d.txt\" %(now.year, now.month, now.day)\n",
    "\n",
    "#get the metoo tweets\n",
    "tweet_list = get_tweets(9000, 'metoo')\n",
    "len(tweet_list)\n",
    "write_Tweets(filename, tweet_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TakeaKnee - an active voice tweet\n",
    "\n",
    "#get the date\n",
    "now = datetime.datetime.now()\n",
    "#set the filename\n",
    "filename = \"takeaknee_static_%d_%d_%d.txt\" %(now.year, now.month, now.day)\n",
    "#get the takeaknee tweets\n",
    "tweet_list = get_tweets(3159, 'takeaknee')\n",
    "len(tweet_list)\n",
    "write_Tweets(filename, tweet_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
